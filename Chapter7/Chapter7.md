#### Chapter 7 Temporal-Difference Methods

TD Learning for State Values, Sarsa, n-step Sarsa, Q-learning, Online/Offline, On-Policy/Off-Policy



1. What is the relationship between TD Learning for State Values, Sarsa, n-step Sarsa, Q-learning?

**TD Learning for State Values** **&** **Sarsa**

2. Derive the formula of TD for state values.

3. Why is v<sub>k</sub>\* the TD target?
4. The formula of Sarsa.
5. Explain the process of using Sarsa and ε-greedy to find optimal policy.
6. What is Expected Sarsa?
7. Explain the converge condition of TD for state values, Sarsa and Q-learning.
8. Compare TD with MC.

**n-step Sarsa**

9. What is the relationship between MC, Sarsa and n-step Sarsa?

10. What happens when n increases?

**Q-learning**

11. The expression of Q-learning
12. The porcess of Q-learning(on-policy and off-policy). Why the policy update methods are difference?

**Online/Offline & On-Policy/Off-Policy**

13. What is online/offline?

14. What is on-policy/off-policy?

15. What is the relationship between online/offline and on-policy/off-policy?



16. Explain the unified viewpoint of MC, Sarsa, n-step Sarsa, Q-learning.





1. TD for state values can only estimate state values used for policy evaluation, but can not find an optimal policy. Sarsa can estimate action values and can be combined with policy improvement steps to find optimal policy. n-step Sarsa is the generalization of Sarsa and MC. Q-learning directly solve BOE to obtain optimal policy.

2. Generate expeirence examples (s<sub>0</sub>, r<sub>1</sub>, s<sub>1</sub>, r<sub>2</sub>,..., s<sub>t</sub>, r<sub>t+1</sub>, s<sub>t+1</sub>, ...) following given policy π. Initially guess     v<sub>0</sub>(s) for every state. To estimate state values: 

<img src="assets/image-20240930104424334.png" alt="image-20240930104424334" style="zoom:50%;" />

​	where v<sub>k+1</sub> is new estimate, v<sub>k</sub> is current estimate, (r<sub>t+1</sub> + γv<sub>k</sub>(s<sub>t+1</sub>)) is TD target *v<sub>k</sub>\**,                                            	(v<sub>k</sub>(s<sub>t</sub>) - (r<sub>t+1</sub> + γv<sub>k</sub>(s<sub>t+1</sub>))) is TD error *δ<sub>k</sub>*.

3. 

<img src="assets/image-20240930111342278.png" alt="image-20240930111342278" style="zoom:50%;" />

4.  Based on the derivation of TD for State Values, Sarsa only needs few changes.

   <img src="assets/image-20240930114941082.png" alt="image-20240930114941082" style="zoom:50%;" />

   Here, s<sub>t+1</sub> and r<sub>t+1</sub> is generated by interacting with the environment. a<sub>t+1</sub> is generated by current policy.

5. 

<img src="assets/image-20240930120916241.png" alt="image-20240930120916241" style="zoom:50%;" />

​	Not all states can gain a optimal policy. TD only focus on finding a optimal path to reach the target state. However, if the expeirence data is enough sufficient, it can still gain optimal policy for every state.

6. Expected sarsa replaces q<sub>k</sub>(s<sub>t+1</sub>, a<sub>t+1</sub>) with E[q<sub>k</sub>(s<sub>t+1</sub>, A)] = Σ<sub>a</sub>π<sub>t</sub>(a|s<sub>t+1</sub>)q<sub>k</sub>(s<sub>t+1</sub>,a) = v<sub>t</sub>(s<sub>t+1</sub>). It can reduce the estimate variance of sarsa for deleting the random variable a<sub>t+1</sub>.
7. The learning rate should decrease but cannot decrease very fast.

<img src="assets/image-20240930113705319.png" alt="image-20240930113705319" style="zoom:33%;" />

​	In the practice of **on-policy** methods, we often set learning rate a small constant because the policy is keep changing and a decaying learning rate may be too small to effectively evaluate the policy. Even a constant learning rate may result in fluctuation, if the constant is small enough, then the flucuation can be neglectable.

8. 

​	(1) TD is incremental. It can update state value/action value immediately after receiving an experience example. MC is non-incremental. It must wait until the whole episode is collected because MC need to calculate the discounted rate of the episode.

​	(2) TD can solve continuing tasks because it is incremental. MC can only solve finite episodes or infinite episode but terminate after finite steps.

​	(3) TD bootstraps. It estimates state values/action values relying on previous estimate. Thus TD need an initial guess. MC is not bootstrapping because it directly calculate state values/action values without initial guess.

​	(4) TD has a lower estimation variance than MC because it needs fewer random variables. Take calculating action value q<sub>π</sub>(s<sub>t</sub>, a<sub>t</sub>) as an example, Sarsa only needs R<sub>t+1</sub>, S<sub>t+1</sub>, A<sub>t+1</sub> but MC needs                       R<sub>t+1</sub>, R<sub>t+2</sub>, R<sub>t+3</sub>, ... .

9. The difference is how to calculate q<sub>π</sub>(s<sub>t</sub>, a<sub>t</sub>). MC and Sarsa are two extreme situation of n-step Sarsa.

<img src="assets/image-20240930140000044.png" alt="image-20240930140000044" style="zoom:50%;" />

10. First, each time updating action value needs a longer time to wait for collecting data (r<sub>t+1</sub>, r<sub>t+2</sub>, r<sub>t+3</sub>, ...., r<sub>t+n</sub>, s<sub>t+n</sub>, a<sub>t+n</sub>). Second, the variance increases but bias decreases.

11. 

<img src="assets/image-20240930145551270.png" alt="image-20240930145551270" style="zoom:50%;" />

12. 

<img src="assets/image-20240930145655304.png" alt="image-20240930145655304" style="zoom:50%;" />

<img src="assets/image-20240930145709533.png" alt="image-20240930145709533" style="zoom:50%;" />

​	For on-policy method, the policy is used in both exporation and update. Thus we need ε-greedy policy for exploring. For off-policy method, π<sub>T</sub> only focus on update without exploring. Thus we just need greedy policy.

13. Online is updating values and policy with interacting with the environment. Offline is learning from generated experience samples following a given policy.

14. The policy which we generate experience samples from is called behavior policy π<sub>b</sub>. The policy which we aim to update to find the optimal policy is called target policy π<sub>T</sub>. For on-policy methods, these two policies are the same. For off-policy methods, these two policies are different. 

​	Sarsa and MC are on-policy, Q-learning can either be on-policy or off-policy.

​	The reason why Q-learning can be off-policy is that it directly solve BOE. Thus the update of q value is 	not related to the policy.

15. If a method is off-policy, then it can use both online and offline and both are same. If a method is on-policy, then it can only use online because the policy needs to be updated in each step.

16. 

<img src="assets/image-20240930152608071.png" alt="image-20240930152608071" style="zoom:50%;" />

<img src="assets/image-20240930152528918.png" alt="image-20240930152528918" style="zoom:50%;" />

